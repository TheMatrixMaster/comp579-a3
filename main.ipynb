{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP579 Assignment 3\n",
    "\n",
    "### Author: Stephen Zhewen Lu\n",
    "### Student ID: 260343328"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Value-based methods with linear function approximation\n",
    "\n",
    "For both experiments, I used:\n",
    "- gamma = 1.0\n",
    "- number of tiles per state dimension = 6\n",
    "- number of tilings = 8\n",
    "\n",
    "For the state tiling code, I was inspired by this [implementation](https://github.com/MeepMoop/tilecoding)\n",
    "\n",
    "## Mountain Car - v0\n",
    "I obtained the following results for the 9 configurations with value-based methods. Generally, I found that smaller learning rate and smaller epsilon resulted in better results with lower variance and convergence to higher average returns. In my results, both algorithms perform quite similarly, with Expected SARSA having a slight edge over QLearning in certain configurations (top row). Given that Mountain Car is a sparse reward environment with complex dynamics, a smaller alpha and smaller epsilon encourages a more stable learning process that favours exploitation over exploration. On the contrary, I found that the largest alpha (0.25) yielded poor results regardless of the epsilon value, indicating that a larger learning rate impedes the algorihms from converging towards a higher reward policy.\n",
    "\n",
    "![mountain-car-value](MountainCar-v0_performance_value_based.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cart Pole - v1\n",
    "For the cart pole environment, I found very similar results where smaller alpha and epsilon lead to better results. Once again, the largest configuration for alpha (0.25) leads to poor results where the average return actually decreases over the length of the episode. Once again, QLearning and Expected SARSA have very similar performance with the latter taking a slight edge on most configurations.\n",
    "\n",
    "![cart-pole-value](CartPole-v1_performance_value_based.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Policy Gradient Theorem\n",
    "\n",
    "![q2](q2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Policy-based methods with linear function approximation\n",
    "\n",
    "For both environments, I used the following hyperparameters:\n",
    "- learning rate (actor) = 0.001\n",
    "- learning rate (critic) = 0.001\n",
    "- gamma = 0.99\n",
    "- fixed temperature = 0.1\n",
    "- temperature decay schedule\n",
    "    - initial temperature = 1.0\n",
    "    - decay tau = 0.995\n",
    "    - min temperature = 0.1\n",
    "    - decay every = \"episode\"\n",
    "\n",
    "For tiling parameters, I used the same values as in Q1.\n",
    "\n",
    "### Mountain Car - v0\n",
    "Here are my results for the 2 configurations (fixed temperature & decaying temperature) for both the REINFORCE and Actor Critic algorithms on the mountain car env. I found that Actor Critic performs very well (better than the value based methods), while REINFORCE performs very poorly. I didn't find much improvement in decaying the temperature parameter instead of keeping it fixed at 0.1\n",
    "\n",
    "In my decay temperature setup, I decay the temperature (initially set to 1.0) by multiplying it by a tau parameter (0.995) after every episode. I also lower bound the temperature at .1 to maintain some exploration.\n",
    "\n",
    "I had a lot of trouble trying to get REINFORCE to work well on the mountain car environment. I implemented both Monte-Carlo and baseline variants and ran a bunch of hyperparameter sweeps without finding much improvement. My hypothesis is that given the sparse nature and complex dynamics of this task, using monte-carlo returns predominantly yields episodes with simply no reward which don't contribute at all to learning. Perhaps, this could be addressed by increasing the number of episodes or modifying the reward to guarantee that the episodes capture some signal.\n",
    "\n",
    "![mountain-car-policy](MountainCar-v0_performance_policy_based.png)\n",
    "\n",
    "\n",
    "### Cart Pole - v1\n",
    "For the cart pole environment, REINFORCE works much better than the mountain car env. This is probably because this environment is somewhat \"easier\" and doesn't have the sparse reward problem. Once again, I found that decaying the temperature didn't have a significant impact. Actor Critic converges to a higher final average reward, but takes longer to do so. Interestingly, I found that these policy based methods achieved lower average return than the value-based methods after 1000 episodes. However, the variance is much lower for policy based methods and the training seems to be more stable. This is aligned with what I'd expect since policy based methods are \"smooth\" and won't change their action selection behaviour drastically from one parameter update to the next.\n",
    "\n",
    "![cart-pole-policy](CartPole-v1_performance_policy_based.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
